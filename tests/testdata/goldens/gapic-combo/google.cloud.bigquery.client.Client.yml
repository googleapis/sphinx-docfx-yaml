### YamlMime:UniversalReference
api_name: []
items:
- attributes: []
  children:
  - google.cloud.bigquery.client.Client.SCOPE
  - google.cloud.bigquery.client.Client.__getstate__
  - google.cloud.bigquery.client.Client.cancel_job
  - google.cloud.bigquery.client.Client.close
  - google.cloud.bigquery.client.Client.copy_table
  - google.cloud.bigquery.client.Client.create_dataset
  - google.cloud.bigquery.client.Client.create_job
  - google.cloud.bigquery.client.Client.create_routine
  - google.cloud.bigquery.client.Client.create_table
  - google.cloud.bigquery.client.Client.dataset
  - google.cloud.bigquery.client.Client.delete_dataset
  - google.cloud.bigquery.client.Client.delete_job_metadata
  - google.cloud.bigquery.client.Client.delete_model
  - google.cloud.bigquery.client.Client.delete_routine
  - google.cloud.bigquery.client.Client.delete_table
  - google.cloud.bigquery.client.Client.extract_table
  - google.cloud.bigquery.client.Client.from_service_account_info
  - google.cloud.bigquery.client.Client.from_service_account_json
  - google.cloud.bigquery.client.Client.get_dataset
  - google.cloud.bigquery.client.Client.get_job
  - google.cloud.bigquery.client.Client.get_model
  - google.cloud.bigquery.client.Client.get_routine
  - google.cloud.bigquery.client.Client.get_service_account_email
  - google.cloud.bigquery.client.Client.get_table
  - google.cloud.bigquery.client.Client.insert_rows
  - google.cloud.bigquery.client.Client.insert_rows_from_dataframe
  - google.cloud.bigquery.client.Client.insert_rows_json
  - google.cloud.bigquery.client.Client.job_from_resource
  - google.cloud.bigquery.client.Client.list_datasets
  - google.cloud.bigquery.client.Client.list_jobs
  - google.cloud.bigquery.client.Client.list_models
  - google.cloud.bigquery.client.Client.list_partitions
  - google.cloud.bigquery.client.Client.list_projects
  - google.cloud.bigquery.client.Client.list_routines
  - google.cloud.bigquery.client.Client.list_rows
  - google.cloud.bigquery.client.Client.list_tables
  - google.cloud.bigquery.client.Client.load_table_from_dataframe
  - google.cloud.bigquery.client.Client.load_table_from_file
  - google.cloud.bigquery.client.Client.load_table_from_json
  - google.cloud.bigquery.client.Client.load_table_from_uri
  - google.cloud.bigquery.client.Client.location
  - google.cloud.bigquery.client.Client.query
  - google.cloud.bigquery.client.Client.schema_from_json
  - google.cloud.bigquery.client.Client.schema_to_json
  - google.cloud.bigquery.client.Client.update_dataset
  - google.cloud.bigquery.client.Client.update_model
  - google.cloud.bigquery.client.Client.update_routine
  - google.cloud.bigquery.client.Client.update_table
  - google.cloud.bigquery.client.Client.__init__
  - google.cloud.bigquery.client.Client.get_iam_policy
  - google.cloud.bigquery.client.Client.set_iam_policy
  - google.cloud.bigquery.client.Client.test_iam_permissions
  - google.cloud.bigquery.client.Client
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client
  inheritance:
  - inheritance:
    - inheritance:
      - inheritance:
        - type: builtins.object
        type: google.cloud.client._ClientFactoryMixin
      type: google.cloud.client.Client
    - inheritance:
      - type: builtins.object
      type: google.cloud.client._ClientProjectMixin
    type: google.cloud.client.ClientWithProject
  langs:
  - python
  module: google.cloud.bigquery.client
  name: Client
  source:
    id: Client
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 170
  summary: 'Client to bundle configuration needed for API requests.

    '
  syntax:
    content: "Client(\n    project=None,\n    credentials=None,\n    _http=None,\n\
      \    location=None,\n    default_query_job_config=None,\n    client_info=None,\n\
      \    client_options=None,\n)"
    exceptions:
    - description: Raised if <code>credentials</code> is not specified and the library
        fails to acquire default credentials.
      var_type: google.auth.exceptions.DefaultCredentialsError
    parameters:
    - description: Project ID for the project which the client acts on behalf of.
        Will be passed when creating a dataset / job. If not passed, falls back to
        the default inferred from the environment.
      id: project
      var_type: Optional[str]
    - description: The OAuth2 Credentials to use for this client. If not passed (and
        if no <code>_http</code> object is passed), falls back to the default inferred
        from the environment.
      id: credentials
      var_type: Optional[google.auth.credentials.Credentials]
    - description: HTTP object to make requests. Can be any object that defines <code>request()</code>
        with the same interface as <code>requests.Session.request</code>. If not passed,
        an <code>_http</code> object is created that is bound to the <code>credentials</code>
        for the current object. This parameter should be considered private, and could
        change in the future.
      id: _http
      var_type: Optional[requests.Session]
    - description: Default location for jobs / datasets / tables.
      id: location
      var_type: Optional[str]
    - description: Default <code>QueryJobConfig</code>. Will be merged into job configs
        passed into the <code>query</code> method.
      id: default_query_job_config
      var_type: Optional[<xref uid="google.cloud.bigquery.job.QueryJobConfig">google.cloud.bigquery.job.QueryJobConfig</xref>]
    - description: The client info used to send a user-agent string along with API
        requests. If <code>None</code>, then default info will be used. Generally,
        you only need to set this if you're developing your own library or partner
        tool.
      id: client_info
      var_type: Optional[google.api_core.client_info.ClientInfo]
    - description: Client options used to set user options on the client. API Endpoint
        should be set through client_options.
      id: client_options
      var_type: Optional[Union[google.api_core.client_options.ClientOptions, Dict]]
  type: class
  uid: google.cloud.bigquery.client.Client
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.SCOPE
  langs:
  - python
  module: google.cloud.bigquery.client
  name: SCOPE
  source:
    id: SCOPE
    path: null
    remote:
      branch: add_goldens
      path: null
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: null
  summary: 'The scopes required for authenticating as a BigQuery consumer.


    '
  syntax: {}
  type: attribute
  uid: google.cloud.bigquery.client.Client.SCOPE
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.__getstate__
  langs:
  - python
  module: google.cloud.bigquery.client
  name: __getstate__
  source:
    id: __getstate__
    path: .tox/update_goldens/lib/python3.9/site-packages/google/cloud/client/__init__.py
    remote:
      branch: add_goldens
      path: .tox/update_goldens/lib/python3.9/site-packages/google/cloud/client/__init__.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 192
  summary: 'Explicitly state that clients are not pickleable.


    '
  syntax:
    content: __getstate__()
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.__getstate__
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.cancel_job
  langs:
  - python
  module: google.cloud.bigquery.client
  name: cancel_job
  source:
    id: cancel_job
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2053
  summary: 'Attempt to cancel a job from a job ID.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/cancel

    '
  syntax:
    content: 'cancel_job(job_id: str, project: typing.Optional[str] = None, location:
      typing.Optional[str] = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    parameters:
    - description: Job identifier.
      id: job_id
      var_type: 'Union[ str, <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>,
        <xref uid="google.cloud.bigquery.job.CopyJob">google.cloud.bigquery.job.CopyJob</xref>,
        <xref uid="google.cloud.bigquery.job.ExtractJob">google.cloud.bigquery.job.ExtractJob</xref>,
        <xref uid="google.cloud.bigquery.job.QueryJob">google.cloud.bigquery.job.QueryJob</xref>
        ] :keyword project: ID of the project which owns the job (defaults to the
        client''s project). :kwtype project: Optional[str] :keyword location: Location
        where the job was run. Ignored if <code>job_id</code> is a job object. :kwtype
        location: Optional[str] :keyword retry: How to retry the RPC. :kwtype retry:
        Optional[google.api_core.retry.Retry] :keyword timeout: The number of seconds
        to wait for the underlying HTTP transport before using <code>retry</code>.
        :kwtype timeout: Optional[float]'
    returns:
    - description: Job instance, based on the resource returned by the API.
      var_type: Union[ <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>,
        <xref uid="google.cloud.bigquery.job.CopyJob">google.cloud.bigquery.job.CopyJob</xref>,
        <xref uid="google.cloud.bigquery.job.ExtractJob">google.cloud.bigquery.job.ExtractJob</xref>,
        <xref uid="google.cloud.bigquery.job.QueryJob">google.cloud.bigquery.job.QueryJob</xref>,
        ]
  type: method
  uid: google.cloud.bigquery.client.Client.cancel_job
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.close
  langs:
  - python
  module: google.cloud.bigquery.client
  name: close
  source:
    id: close
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 254
  summary: 'Close the underlying transport objects, releasing system resources.


    <aside class="note">

    <b>Note:</b>

    The client instance can be used for making additional requests even

    after closing, in which case the underlying connections are

    automatically re-created.


    </aside>'
  syntax:
    content: close()
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.close
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.copy_table
  langs:
  - python
  module: google.cloud.bigquery.client
  name: copy_table
  source:
    id: copy_table
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2985
  summary: 'Copy one or more tables to another table.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationtablecopy

    '
  syntax:
    content: 'copy_table(sources: typing.Union[google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem,
      str, typing.Sequence[typing.Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference,
      google.cloud.bigquery.table.TableListItem, str]]], destination: typing.Union[google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem,
      str], job_id: typing.Optional[str] = None, job_id_prefix: typing.Optional[str]
      = None, location: typing.Optional[str] = None, project: typing.Optional[str]
      = None, job_config: typing.Optional[google.cloud.bigquery.job.copy_.CopyJobConfig]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None)'
    exceptions:
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.CopyJobConfig">CopyJobConfig</xref>
        class.
      var_type: TypeError
    parameters:
    - description: Table or tables to be copied.
      id: sources
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, Sequence[ Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ] ], ]
    - description: Table into which data is to be copied.
      id: destination
      var_type: 'Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ] :keyword job_id: The ID of the job. :kwtype job_id: Optional[str] :keyword
        job_id_prefix: The user-provided prefix for a randomly generated job ID. This
        parameter will be ignored if a <code>job_id</code> is also given. :kwtype
        job_id_prefix: Optional[str] :keyword location: Location where to run the
        job. Must match the location of any source table as well as the destination
        table. :kwtype location: Optional[str] :keyword project: Project ID of the
        project of where to run the job. Defaults to the client''s project. :kwtype
        project: Optional[str] :keyword job_config: Extra configuration options for
        the job. :kwtype job_config: Optional[<xref uid="google.cloud.bigquery.job.CopyJobConfig">google.cloud.bigquery.job.CopyJobConfig</xref>]
        :keyword retry: How to retry the RPC. :kwtype retry: Optional[google.api_core.retry.Retry]
        :keyword timeout: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>. :kwtype timeout: Optional[float]'
    returns:
    - description: A new copy job instance.
      var_type: <xref uid="google.cloud.bigquery.job.CopyJob">google.cloud.bigquery.job.CopyJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.copy_table
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.create_dataset
  langs:
  - python
  module: google.cloud.bigquery.client
  name: create_dataset
  source:
    id: create_dataset
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 549
  summary: 'API call: create the dataset via a POST request.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/insert

    '
  syntax:
    content: 'create_dataset(dataset: typing.Union[str, google.cloud.bigquery.dataset.Dataset,
      google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem],
      exists_ok: bool = False, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    exceptions:
    - description: 'If the dataset already exists. .. rubric:: Example >>> from google.cloud
        import bigquery >>> client = bigquery.Client() >>> dataset = bigquery.Dataset(''my_project.my_dataset'')
        >>> dataset = client.create_dataset(dataset)'
      var_type: google.cloud.exceptions.Conflict
    parameters:
    - description: A <xref uid="google.cloud.bigquery.dataset.Dataset">Dataset</xref>
        to create. If <code>dataset</code> is a reference, an empty dataset is created
        with the specified ID and client's default location.
      id: dataset
      var_type: Union[ <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetListItem">google.cloud.bigquery.dataset.DatasetListItem</xref>,
        str, ]
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "already
        exists" errors when creating the dataset.
      id: exists_ok
      var_type: Optional[bool]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A new <code>Dataset</code> returned from the API.
      var_type: <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.create_dataset
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.create_job
  langs:
  - python
  module: google.cloud.bigquery.client
  name: create_job
  source:
    id: create_job
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1894
  summary: 'Create a new job.


    '
  syntax:
    content: 'create_job(job_config: dict, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    parameters:
    - description: configuration job representation returned from the API.
      id: job_config
      var_type: 'dict :keyword retry: How to retry the RPC. :kwtype retry: Optional[google.api_core.retry.Retry]
        :keyword timeout: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>. :kwtype timeout: Optional[float]'
    returns:
    - description: A new job instance.
      var_type: Union[ <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>,
        <xref uid="google.cloud.bigquery.job.CopyJob">google.cloud.bigquery.job.CopyJob</xref>,
        <xref uid="google.cloud.bigquery.job.ExtractJob">google.cloud.bigquery.job.ExtractJob</xref>,
        <xref uid="google.cloud.bigquery.job.QueryJob">google.cloud.bigquery.job.QueryJob</xref>
        ]
  type: method
  uid: google.cloud.bigquery.client.Client.create_job
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.create_routine
  langs:
  - python
  module: google.cloud.bigquery.client
  name: create_routine
  source:
    id: create_routine
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 624
  summary: '[Beta] Create a routine via a POST request.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/routines/insert

    '
  syntax:
    content: 'create_routine(routine: google.cloud.bigquery.routine.routine.Routine,
      exists_ok: bool = False, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    exceptions:
    - description: If the routine already exists.
      var_type: google.cloud.exceptions.Conflict
    parameters:
    - description: A <xref uid="google.cloud.bigquery.routine.Routine">Routine</xref>
        to create. The dataset that the routine belongs to must already exist.
      id: routine
      var_type: <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "already
        exists" errors when creating the routine.
      id: exists_ok
      var_type: Optional[bool]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A new <code>Routine</code> returned from the service.
      var_type: <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.create_routine
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.create_table
  langs:
  - python
  module: google.cloud.bigquery.client
  name: create_table
  source:
    id: create_table
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 679
  summary: 'API call:  create a table via a PUT request


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/insert

    '
  syntax:
    content: 'create_table(table: typing.Union[str, google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem],
      exists_ok: bool = False, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    exceptions:
    - description: If the table already exists.
      var_type: google.cloud.exceptions.Conflict
    parameters:
    - description: A <xref uid="google.cloud.bigquery.table.Table">Table</xref> to
        create. If <code>table</code> is a reference, an empty table is created with
        the specified ID. The dataset that the table belongs to must already exist.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ]
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "already
        exists" errors when creating the table.
      id: exists_ok
      var_type: Optional[bool]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A new <code>Table</code> returned from the service.
      var_type: <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.create_table
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.dataset
  langs:
  - python
  module: google.cloud.bigquery.client
  name: dataset
  source:
    id: dataset
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 458
  summary: "Deprecated: Construct a reference to a dataset.\n\n<aside class=\"deprecated\"\
    >\n<b>deprecated:</b>\nConstruct a\nxref_DatasetReference using its\nconstructor\
    \ or use a string where previously a reference object\nwas used.\n</aside>\n \
    \  As of `google-cloud-bigquery` version 1.7.0, all client methods\n   that take\
    \ a\n   xref_DatasetReference or\n   xref_TableReference also take a\n   string\
    \ in standard SQL format, e.g. `project.dataset_id` or\n   `project.dataset_id.table_id`.\n"
  syntax:
    content: 'dataset(dataset_id: str, project: typing.Optional[str] = None)'
    parameters:
    - description: ID of the dataset.
      id: dataset_id
      var_type: str
    - defaultValue: None
      description: Project ID for the dataset (defaults to the project of the client).
      id: project
      var_type: Optional[str]
    returns:
    - description: a new <code>DatasetReference</code> instance.
      var_type: <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.dataset
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.delete_dataset
  langs:
  - python
  module: google.cloud.bigquery.client
  name: delete_dataset
  source:
    id: delete_dataset
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1524
  summary: 'Delete a dataset.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/delete

    '
  syntax:
    content: 'delete_dataset(dataset: typing.Union[google.cloud.bigquery.dataset.Dataset,
      google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem,
      str], delete_contents: bool = False, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None, not_found_ok: bool = False)'
    parameters:
    - description: A reference to the dataset to delete. If a string is passed in,
        this method attempts to create a dataset reference from a string using <xref
        uid="google.cloud.bigquery.dataset.DatasetReference.from_string">from_string</xref>.
      id: dataset
      var_type: Union[ <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetListItem">google.cloud.bigquery.dataset.DatasetListItem</xref>,
        str, ]
    - defaultValue: 'False'
      description: If True, delete all the tables in the dataset. If False and the
        dataset contains tables, the request will fail. Default is False.
      id: delete_contents
      var_type: Optional[bool]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "not
        found" errors when deleting the dataset.
      id: not_found_ok
      var_type: Optional[bool]
  type: method
  uid: google.cloud.bigquery.client.Client.delete_dataset
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.delete_job_metadata
  langs:
  - python
  module: google.cloud.bigquery.client
  name: delete_job_metadata
  source:
    id: delete_job_metadata
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1636
  summary: '[Beta] Delete job metadata from job history.


    Note: This does not stop a running job. Use

    xref_cancel_job instead.

    '
  syntax:
    content: 'delete_job_metadata(job_id: typing.Union[str, google.cloud.bigquery.job.load.LoadJob,
      google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob,
      google.cloud.bigquery.job.query.QueryJob], project: typing.Optional[str] = None,
      location: typing.Optional[str] = None, retry: google.api_core.retry.Retry =
      <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None,
      not_found_ok: bool = False)'
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.delete_job_metadata
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.delete_model
  langs:
  - python
  module: google.cloud.bigquery.client
  name: delete_model
  source:
    id: delete_model
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1584
  summary: '[Beta] Delete a model


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/models/delete

    '
  syntax:
    content: 'delete_model(model: typing.Union[google.cloud.bigquery.model.Model,
      google.cloud.bigquery.model.ModelReference, str], retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None,
      not_found_ok: bool = False)'
    parameters:
    - description: A reference to the model to delete. If a string is passed in, this
        method attempts to create a model reference from a string using <xref uid="google.cloud.bigquery.model.ModelReference.from_string">from_string</xref>.
      id: model
      var_type: Union[ <xref uid="google.cloud.bigquery.model.Model">google.cloud.bigquery.model.Model</xref>,
        <xref uid="google.cloud.bigquery.model.ModelReference">google.cloud.bigquery.model.ModelReference</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "not
        found" errors when deleting the model.
      id: not_found_ok
      var_type: Optional[bool]
  type: method
  uid: google.cloud.bigquery.client.Client.delete_model
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.delete_routine
  langs:
  - python
  module: google.cloud.bigquery.client
  name: delete_routine
  source:
    id: delete_routine
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1701
  summary: '[Beta] Delete a routine.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/routines/delete

    '
  syntax:
    content: 'delete_routine(routine: typing.Union[google.cloud.bigquery.routine.routine.Routine,
      google.cloud.bigquery.routine.routine.RoutineReference, str], retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None,
      not_found_ok: bool = False)'
    parameters:
    - description: A reference to the routine to delete. If a string is passed in,
        this method attempts to create a routine reference from a string using <xref
        uid="google.cloud.bigquery.routine.RoutineReference.from_string">from_string</xref>.
      id: routine
      var_type: Union[ <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>,
        <xref uid="google.cloud.bigquery.routine.RoutineReference">google.cloud.bigquery.routine.RoutineReference</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "not
        found" errors when deleting the routine.
      id: not_found_ok
      var_type: Optional[bool]
  type: method
  uid: google.cloud.bigquery.client.Client.delete_routine
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.delete_table
  langs:
  - python
  module: google.cloud.bigquery.client
  name: delete_table
  source:
    id: delete_table
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1755
  summary: 'Delete a table


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/delete

    '
  syntax:
    content: 'delete_table(table: typing.Union[google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem,
      str], retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, not_found_ok: bool = False)'
    parameters:
    - description: A reference to the table to delete. If a string is passed in, this
        method attempts to create a table reference from a string using <xref uid="google.cloud.bigquery.table.TableReference.from_string">from_string</xref>.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: 'False'
      description: Defaults to <code>False</code>. If <code>True</code>, ignore "not
        found" errors when deleting the table.
      id: not_found_ok
      var_type: Optional[bool]
  type: method
  uid: google.cloud.bigquery.client.Client.delete_table
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.extract_table
  langs:
  - python
  module: google.cloud.bigquery.client
  name: extract_table
  source:
    id: extract_table
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3097
  summary: 'Start a job to extract a table into Cloud Storage files.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationextract

    '
  syntax:
    content: 'extract_table(source: typing.Union[google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem,
      google.cloud.bigquery.model.Model, google.cloud.bigquery.model.ModelReference,
      str], destination_uris: typing.Union[str, typing.Sequence[str]], job_id: typing.Optional[str]
      = None, job_id_prefix: typing.Optional[str] = None, location: typing.Optional[str]
      = None, project: typing.Optional[str] = None, job_config: typing.Optional[google.cloud.bigquery.job.extract.ExtractJobConfig]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, source_type: str = ''Table'')'
    exceptions:
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.ExtractJobConfig">ExtractJobConfig</xref>
        class.
      var_type: TypeError
    - description: If <code>source_type</code> is not among <code>Table</code>,<code>Model</code>.
      var_type: ValueError
    parameters:
    - description: Table or Model to be extracted.
      id: source
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        <xref uid="google.cloud.bigquery.model.Model">google.cloud.bigquery.model.Model</xref>,
        <xref uid="google.cloud.bigquery.model.ModelReference">google.cloud.bigquery.model.ModelReference</xref>,
        src, ]
    - description: URIs of Cloud Storage file(s) into which table data is to be extracted;
        in format <code>gs://<bucket_name>/<object_name_or_glob></code>.
      id: destination_uris
      var_type: 'Union[str, Sequence[str]] :keyword job_id: The ID of the job. :kwtype
        job_id: Optional[str] :keyword job_id_prefix: The user-provided prefix for
        a randomly generated job ID. This parameter will be ignored if a <code>job_id</code>
        is also given. :kwtype job_id_prefix: Optional[str] :keyword location: Location
        where to run the job. Must match the location of the source table. :kwtype
        location: Optional[str] :keyword project: Project ID of the project of where
        to run the job. Defaults to the client''s project. :kwtype project: Optional[str]
        :keyword job_config: Extra configuration options for the job. :kwtype job_config:
        Optional[<xref uid="google.cloud.bigquery.job.ExtractJobConfig">google.cloud.bigquery.job.ExtractJobConfig</xref>]
        :keyword retry: How to retry the RPC. :kwtype retry: Optional[google.api_core.retry.Retry]
        :keyword timeout: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>. :kwtype timeout: Optional[float] :keyword
        source_type: Type of source to be extracted.<code>Table</code> or <code>Model</code>.
        Defaults to <code>Table</code>. :kwtype source_type: Optional[str]'
    returns:
    - description: A new extract job instance.
      var_type: <xref uid="google.cloud.bigquery.job.ExtractJob">google.cloud.bigquery.job.ExtractJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.extract_table
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.from_service_account_info
  langs:
  - python
  module: google.cloud.bigquery.client
  name: from_service_account_info
  source:
    id: from_service_account_info
    path: .tox/update_goldens/lib/python3.9/site-packages/google/cloud/client/__init__.py
    remote:
      branch: add_goldens
      path: .tox/update_goldens/lib/python3.9/site-packages/google/cloud/client/__init__.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 54
  summary: 'Factory to retrieve JSON credentials while creating client.

    '
  syntax:
    content: from_service_account_info(info, *args, **kwargs)
    exceptions:
    - description: if there is a conflict with the kwargs and the credentials created
        by the factory.
      var_type: TypeError
    parameters:
    - description: Remaining positional arguments to pass to constructor.
      id: args
      var_type: tuple
    - description: The JSON object with a private key and other credentials information
        (downloaded from the Google APIs console).
      id: info
      var_type: dict
    returns:
    - description: The client created with the retrieved JSON credentials.
      var_type: <code>_ClientFactoryMixin</code>
  type: method
  uid: google.cloud.bigquery.client.Client.from_service_account_info
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.from_service_account_json
  langs:
  - python
  module: google.cloud.bigquery.client
  name: from_service_account_json
  source:
    id: from_service_account_json
    path: .tox/update_goldens/lib/python3.9/site-packages/google/cloud/client/__init__.py
    remote:
      branch: add_goldens
      path: .tox/update_goldens/lib/python3.9/site-packages/google/cloud/client/__init__.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 84
  summary: 'Factory to retrieve JSON credentials while creating client.

    '
  syntax:
    content: from_service_account_json(json_credentials_path, *args, **kwargs)
    exceptions:
    - description: if there is a conflict with the kwargs and the credentials created
        by the factory.
      var_type: TypeError
    parameters:
    - description: Remaining positional arguments to pass to constructor.
      id: args
      var_type: tuple
    - description: The path to a private key file (this file was given to you when
        you created the service account). This file must contain a JSON object with
        a private key and other credentials information (downloaded from the Google
        APIs console).
      id: json_credentials_path
      var_type: str
    returns:
    - description: The client created with the retrieved JSON credentials.
      var_type: <code>_ClientFactoryMixin</code>
  type: method
  uid: google.cloud.bigquery.client.Client.from_service_account_json
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_dataset
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_dataset
  source:
    id: get_dataset
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 763
  summary: 'Fetch the dataset referenced by `dataset_ref`

    '
  syntax:
    content: 'get_dataset(dataset_ref: typing.Union[google.cloud.bigquery.dataset.DatasetReference,
      str], retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None)'
    parameters:
    - description: A reference to the dataset to fetch from the BigQuery API. If a
        string is passed in, this method attempts to create a dataset reference from
        a string using <xref uid="google.cloud.bigquery.dataset.DatasetReference.from_string">from_string</xref>.
      id: dataset_ref
      var_type: Union[ <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A <code>Dataset</code> instance.
      var_type: <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.get_dataset
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_job
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_job
  source:
    id: get_job
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1990
  summary: 'Fetch a job for the project associated with this client.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/get

    '
  syntax:
    content: 'get_job(job_id: typing.Union[str, google.cloud.bigquery.job.load.LoadJob,
      google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob,
      google.cloud.bigquery.job.query.QueryJob], project: typing.Optional[str] = None,
      location: typing.Optional[str] = None, retry: google.api_core.retry.Retry =
      <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None)'
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.get_job
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_model
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_model
  source:
    id: get_model
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 892
  summary: '[Beta] Fetch the model referenced by `model_ref`.

    '
  syntax:
    content: 'get_model(model_ref: typing.Union[google.cloud.bigquery.model.ModelReference,
      str], retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None)'
    parameters:
    - description: A reference to the model to fetch from the BigQuery API. If a string
        is passed in, this method attempts to create a model reference from a string
        using <xref uid="google.cloud.bigquery.model.ModelReference.from_string">from_string</xref>.
      id: model_ref
      var_type: Union[ <xref uid="google.cloud.bigquery.model.ModelReference">google.cloud.bigquery.model.ModelReference</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A <code>Model</code> instance.
      var_type: <xref uid="google.cloud.bigquery.model.Model">google.cloud.bigquery.model.Model</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.get_model
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_routine
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_routine
  source:
    id: get_routine
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 935
  summary: '[Beta] Get the routine referenced by `routine_ref`.

    '
  syntax:
    content: 'get_routine(routine_ref: typing.Union[google.cloud.bigquery.routine.routine.Routine,
      google.cloud.bigquery.routine.routine.RoutineReference, str], retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None)'
    parameters:
    - description: A reference to the routine to fetch from the BigQuery API. If a
        string is passed in, this method attempts to create a reference from a string
        using <xref uid="google.cloud.bigquery.routine.RoutineReference.from_string">from_string</xref>.
      id: routine_ref
      var_type: Union[ <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>,
        <xref uid="google.cloud.bigquery.routine.RoutineReference">google.cloud.bigquery.routine.RoutineReference</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the API call.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A <code>Routine</code> instance.
      var_type: <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.get_routine
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_service_account_email
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_service_account_email
  source:
    id: get_service_account_email
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 266
  summary: 'Get the email address of the project''s BigQuery service account


    <aside class="note">

    <b>Note:</b>

    This is the service account that BigQuery uses to manage tables

    encrypted by a key in KMS.

    </aside>'
  syntax:
    content: 'get_service_account_email(project: typing.Optional[str] = None, retry:
      google.api_core.retry.Retry = <google.api_core.retry.Retry object>, timeout:
      typing.Optional[float] = None)'
    parameters:
    - defaultValue: None
      description: Project ID to use for retreiving service account email. Defaults
        to the client's project.
      id: project
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: service account email address
      var_type: 'str .. rubric:: Example >>> from google.cloud import bigquery >>>
        client = bigquery.Client() >>> client.get_service_account_email() my_service_account@my-project.iam.gserviceaccount.com'
  type: method
  uid: google.cloud.bigquery.client.Client.get_service_account_email
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_table
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_table
  source:
    id: get_table
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 979
  summary: 'Fetch the table referenced by `table`.

    '
  syntax:
    content: 'get_table(table: typing.Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference,
      google.cloud.bigquery.table.TableListItem, str], retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None)'
    parameters:
    - description: A reference to the table to fetch from the BigQuery API. If a string
        is passed in, this method attempts to create a table reference from a string
        using <xref uid="google.cloud.bigquery.table.TableReference.from_string">from_string</xref>.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A <code>Table</code> instance.
      var_type: <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.get_table
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.insert_rows
  langs:
  - python
  module: google.cloud.bigquery.client
  name: insert_rows
  source:
    id: insert_rows
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3349
  summary: 'Insert rows into a table via the streaming API.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll

    '
  syntax:
    content: "insert_rows(\n    table: typing.Union[\n        google.cloud.bigquery.table.Table,\n\
      \        google.cloud.bigquery.table.TableReference,\n        str,\n    ],\n\
      \    rows: typing.Union[typing.Iterable[typing.Tuple], typing.Iterable[typing.Dict]],\n\
      \    selected_fields: typing.Optional[\n        typing.Sequence[google.cloud.bigquery.schema.SchemaField]\n\
      \    ] = None,\n    **kwargs\n)"
    exceptions:
    - description: if table's schema is not set or <code>rows</code> is not a <code>Sequence</code>.
      var_type: ValueError
    parameters:
    - description: The destination table for the row data, or a reference to it.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        str, ]
    - description: Row data to be inserted. If a list of tuples is given, each tuple
        should contain data for each schema field on the current table and in the
        same order as the schema fields. If a list of dictionaries is given, the keys
        must include all required fields in the schema. Keys which do not correspond
        to a field in the schema are ignored.
      id: rows
      var_type: Union[Sequence[Tuple], Sequence[Dict]]
    - description: The fields to return. Required if <code>table</code> is a <xref
        uid="google.cloud.bigquery.table.TableReference">TableReference</xref>.
      id: selected_fields
      var_type: Sequence[<xref uid="google.cloud.bigquery.schema.SchemaField">google.cloud.bigquery.schema.SchemaField</xref>]
    - defaultValue: None
      description: Keyword arguments to <xref uid="google.cloud.bigquery.client.Client.insert_rows_json">insert_rows_json</xref>.
      id: kwargs
      var_type: dict
    returns:
    - description: 'One mapping per row with insert errors: the "index" key identifies
        the row, and the "errors" key contains a list of the mappings describing one
        or more problems with the row.'
      var_type: Sequence[Mappings]
  type: method
  uid: google.cloud.bigquery.client.Client.insert_rows
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.insert_rows_from_dataframe
  langs:
  - python
  module: google.cloud.bigquery.client
  name: insert_rows_from_dataframe
  source:
    id: insert_rows_from_dataframe
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3417
  summary: 'Insert rows into a table from a dataframe via the streaming API.

    '
  syntax:
    content: "insert_rows_from_dataframe(\n    table: typing.Union[\n        google.cloud.bigquery.table.Table,\n\
      \        google.cloud.bigquery.table.TableReference,\n        str,\n    ],\n\
      \    dataframe,\n    selected_fields: typing.Optional[\n        typing.Sequence[google.cloud.bigquery.schema.SchemaField]\n\
      \    ] = None,\n    chunk_size: int = 500,\n    **kwargs: typing.Dict\n)"
    exceptions:
    - description: if table's schema is not set
      var_type: ValueError
    parameters:
    - description: The destination table for the row data, or a reference to it.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        str, ]
    - description: The fields to return. Required if <code>table</code> is a <xref
        uid="google.cloud.bigquery.table.TableReference">TableReference</xref>.
      id: selected_fields
      var_type: Sequence[<xref uid="google.cloud.bigquery.schema.SchemaField">google.cloud.bigquery.schema.SchemaField</xref>]
    - defaultValue: None
      description: The number of rows to stream in a single chunk. Must be positive.
      id: chunk_size
      var_type: int
    - defaultValue: '500'
      description: Keyword arguments to <xref uid="google.cloud.bigquery.client.Client.insert_rows_json">insert_rows_json</xref>.
      id: kwargs
      var_type: Dict
    - description: A <code>pandas.DataFrame</code> containing the data to load. Any
        <code>NaN</code> values present in the dataframe are omitted from the streaming
        API request(s).
      id: dataframe
      var_type: pandas.DataFrame
    returns:
    - description: 'A list with insert errors for each insert chunk. Each element
        is a list containing one mapping per row with insert errors: the "index" key
        identifies the row, and the "errors" key contains a list of the mappings describing
        one or more problems with the row.'
      var_type: Sequence[Sequence[Mappings]]
  type: method
  uid: google.cloud.bigquery.client.Client.insert_rows_from_dataframe
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.insert_rows_json
  langs:
  - python
  module: google.cloud.bigquery.client
  name: insert_rows_json
  source:
    id: insert_rows_json
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3470
  summary: 'Insert rows into a table without applying local type conversions.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll

    '
  syntax:
    content: 'insert_rows_json(table: typing.Union[google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem,
      str], json_rows: typing.Sequence[typing.Dict], row_ids: typing.Optional[typing.Union[typing.Iterable[typing.Optional[str]],
      google.cloud.bigquery.enums.AutoRowIDs]] = AutoRowIDs.GENERATE_UUID, skip_invalid_rows:
      typing.Optional[bool] = None, ignore_unknown_values: typing.Optional[bool] =
      None, template_suffix: typing.Optional[str] = None, retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None)'
    exceptions:
    - description: if <code>json_rows</code> is not a <code>Sequence</code>.
      var_type: TypeError
    parameters:
    - description: The destination table for the row data, or a reference to it.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str ]
    - description: Row data to be inserted. Keys must match the table schema fields
        and values must be JSON-compatible representations.
      id: json_rows
      var_type: Sequence[Dict]
    - defaultValue: AutoRowIDs.GENERATE_UUID
      description: 'Unique IDs, one per row being inserted. An ID can also be <code>None</code>,
        indicating that an explicit insert ID should **not** be used for that row.
        If the argument is omitted altogether, unique IDs are created automatically.
        .. versionchanged:: 2.21.0 Can also be an iterable, not just a sequence, or
        an <code>AutoRowIDs</code> enum member. .. deprecated:: 2.21.0 Passing <code>None</code>
        to explicitly request autogenerating insert IDs is deprecated, use <code>AutoRowIDs.GENERATE_UUID</code>
        instead.'
      id: row_ids
      var_type: Union[Iterable[str], AutoRowIDs, None]
    - defaultValue: None
      description: Insert all valid rows of a request, even if invalid rows exist.
        The default value is <code>False</code>, which causes the entire request to
        fail if any invalid rows exist.
      id: skip_invalid_rows
      var_type: Optional[bool]
    - defaultValue: None
      description: Accept rows that contain values that do not match the schema. The
        unknown values are ignored. Default is <code>False</code>, which treats unknown
        values as errors.
      id: ignore_unknown_values
      var_type: Optional[bool]
    - defaultValue: None
      description: Treat <code>name</code> as a template table and provide a suffix.
        BigQuery will create the table <code><name> + <template_suffix></code> based
        on the schema of the template table. See https://cloud.google.com/bigquery/streaming-data-into-bigquery#template-tables
      id: template_suffix
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: 'One mapping per row with insert errors: the "index" key identifies
        the row, and the "errors" key contains a list of the mappings describing one
        or more problems with the row.'
      var_type: Sequence[Mappings]
  type: method
  uid: google.cloud.bigquery.client.Client.insert_rows_json
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.job_from_resource
  langs:
  - python
  module: google.cloud.bigquery.client
  name: job_from_resource
  source:
    id: job_from_resource
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1872
  summary: 'Detect correct job type from resource and instantiate.

    '
  syntax:
    content: 'job_from_resource(resource: dict)'
    parameters:
    - description: one job resource from API response
      id: resource
      var_type: Dict
  type: method
  uid: google.cloud.bigquery.client.Client.job_from_resource
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_datasets
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_datasets
  source:
    id: list_datasets
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 375
  summary: 'List datasets for the project associated with this client.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list

    '
  syntax:
    content: 'list_datasets(project: typing.Optional[str] = None, include_all: bool
      = False, filter: typing.Optional[str] = None, max_results: typing.Optional[int]
      = None, page_token: typing.Optional[str] = None, retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, timeout: typing.Optional[float] = None,
      page_size: typing.Optional[int] = None)'
    parameters:
    - defaultValue: None
      description: Project ID to use for retreiving datasets. Defaults to the client's
        project.
      id: project
      var_type: Optional[str]
    - defaultValue: 'False'
      description: True if results include hidden datasets. Defaults to False.
      id: include_all
      var_type: Optional[bool]
    - defaultValue: None
      description: An expression for filtering the results by label. For syntax, see
        https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list#body.QUERY_PARAMETERS.filter
      id: filter
      var_type: Optional[str]
    - defaultValue: None
      description: Maximum number of datasets to return.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Token representing a cursor into the datasets. If not passed, the
        API will return the first page of datasets. The token marks the beginning
        of the iterator to be returned and the value of the <code>page_token</code>
        can be accessed at <code>next_page_token</code> of the <code>google.api_core.page_iterator.HTTPIterator</code>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: None
      description: Maximum number of datasets to return per page.
      id: page_size
      var_type: Optional[int]
    returns:
    - description: Iterator of <xref uid="google.cloud.bigquery.dataset.DatasetListItem">DatasetListItem</xref>.
        associated with the project.
      var_type: google.api_core.page_iterator.Iterator
  type: method
  uid: google.cloud.bigquery.client.Client.list_datasets
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_jobs
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_jobs
  source:
    id: list_jobs
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2132
  summary: 'List jobs for the project associated with this client.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/list

    '
  syntax:
    content: 'list_jobs(project: typing.Optional[str] = None, parent_job: typing.Optional[typing.Union[google.cloud.bigquery.job.query.QueryJob,
      str]] = None, max_results: typing.Optional[int] = None, page_token: typing.Optional[str]
      = None, all_users: typing.Optional[bool] = None, state_filter: typing.Optional[str]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, min_creation_time: typing.Optional[datetime.datetime]
      = None, max_creation_time: typing.Optional[datetime.datetime] = None, page_size:
      typing.Optional[int] = None)'
    parameters:
    - defaultValue: None
      description: Project ID to use for retreiving datasets. Defaults to the client's
        project.
      id: project
      var_type: Optional[str]
    - defaultValue: None
      description: If set, retrieve only child jobs of the specified parent.
      id: parent_job
      var_type: Optional[Union[ google.cloud.bigquery.job._AsyncJob, str, ]]
    - defaultValue: None
      description: Maximum number of jobs to return.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Opaque marker for the next "page" of jobs. If not passed, the API
        will return the first page of jobs. The token marks the beginning of the iterator
        to be returned and the value of the <code>page_token</code> can be accessed
        at <code>next_page_token</code> of <code>google.api_core.page_iterator.HTTPIterator</code>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: None
      description: If true, include jobs owned by all users in the project. Defaults
        to :data:<code>False</code>.
      id: all_users
      var_type: Optional[bool]
    - defaultValue: None
      description: 'If set, include only jobs matching the given state. One of: *
        <code>"done"</code> * <code>"pending"</code> * <code>"running"</code>'
      id: state_filter
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: None
      description: Min value for job creation time. If set, only jobs created after
        or at this timestamp are returned. If the datetime has no time zone assumes
        UTC time.
      id: min_creation_time
      var_type: Optional[datetime.datetime]
    - defaultValue: None
      description: Max value for job creation time. If set, only jobs created before
        or at this timestamp are returned. If the datetime has no time zone assumes
        UTC time.
      id: max_creation_time
      var_type: Optional[datetime.datetime]
    - defaultValue: None
      description: Maximum number of jobs to return per page.
      id: page_size
      var_type: Optional[int]
    returns:
    - description: Iterable of job instances.
      var_type: google.api_core.page_iterator.Iterator
  type: method
  uid: google.cloud.bigquery.client.Client.list_jobs
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_models
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_models
  source:
    id: list_models
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1294
  summary: '[Beta] List models in the dataset.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/models/list

    '
  syntax:
    content: 'list_models(dataset: typing.Union[google.cloud.bigquery.dataset.Dataset,
      google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem,
      str], max_results: typing.Optional[int] = None, page_token: typing.Optional[str]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, page_size: typing.Optional[int] = None)'
    parameters:
    - description: A reference to the dataset whose models to list from the BigQuery
        API. If a string is passed in, this method attempts to create a dataset reference
        from a string using <xref uid="google.cloud.bigquery.dataset.DatasetReference.from_string">from_string</xref>.
      id: dataset
      var_type: Union[ <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetListItem">google.cloud.bigquery.dataset.DatasetListItem</xref>,
        str, ]
    - defaultValue: None
      description: Maximum number of models to return. Defaults to a value set by
        the API.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Token representing a cursor into the models. If not passed, the
        API will return the first page of models. The token marks the beginning of
        the iterator to be returned and the value of the <code>page_token</code> can
        be accessed at <code>next_page_token</code> of the <code>google.api_core.page_iterator.HTTPIterator</code>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: None
      description: Maximum number of models to return per page. Defaults to a value
        set by the API.
      id: page_size
      var_type: 'Optional[int] Returns: google.api_core.page_iterator.Iterator: Iterator
        of <xref uid="google.cloud.bigquery.model.Model">Model</xref> contained within
        the requested dataset.'
  type: method
  uid: google.cloud.bigquery.client.Client.list_models
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_partitions
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_partitions
  source:
    id: list_partitions
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3613
  summary: 'List the partitions in a table.

    '
  syntax:
    content: 'list_partitions(table: typing.Union[google.cloud.bigquery.table.Table,
      google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem,
      str], retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None)'
    parameters:
    - description: The table or reference from which to get partition info
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>. If multiple requests are made under the hood,
        <code>timeout</code> applies to each individual request.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: A list of the partition ids present in the partitioned table
      var_type: List[str]
  type: method
  uid: google.cloud.bigquery.client.Client.list_partitions
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_projects
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_projects
  source:
    id: list_projects
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 312
  summary: 'List projects for the project associated with this client.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/projects/list

    '
  syntax:
    content: 'list_projects(max_results: typing.Optional[int] = None, page_token:
      typing.Optional[str] = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None, page_size: typing.Optional[int]
      = None)'
    parameters:
    - defaultValue: None
      description: Maximum number of projects to return. Defaults to a value set by
        the API.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Token representing a cursor into the projects. If not passed, the
        API will return the first page of projects. The token marks the beginning
        of the iterator to be returned and the value of the <code>page_token</code>
        can be accessed at <code>next_page_token</code> of the <code>google.api_core.page_iterator.HTTPIterator</code>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: None
      description: Maximum number of projects to return in each page. Defaults to
        a value set by the API.
      id: page_size
      var_type: Optional[int]
    returns:
    - description: Iterator of <xref uid="google.cloud.bigquery.client.Project">Project</xref>
        accessible to the current client.
      var_type: google.api_core.page_iterator.Iterator
  type: method
  uid: google.cloud.bigquery.client.Client.list_projects
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_routines
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_routines
  source:
    id: list_routines
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1371
  summary: '[Beta] List routines in the dataset.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/routines/list

    '
  syntax:
    content: 'list_routines(dataset: typing.Union[google.cloud.bigquery.dataset.Dataset,
      google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem,
      str], max_results: typing.Optional[int] = None, page_token: typing.Optional[str]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, page_size: typing.Optional[int] = None)'
    parameters:
    - description: A reference to the dataset whose routines to list from the BigQuery
        API. If a string is passed in, this method attempts to create a dataset reference
        from a string using <xref uid="google.cloud.bigquery.dataset.DatasetReference.from_string">from_string</xref>.
      id: dataset
      var_type: Union[ <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetListItem">google.cloud.bigquery.dataset.DatasetListItem</xref>,
        str, ]
    - defaultValue: None
      description: Maximum number of routines to return. Defaults to a value set by
        the API.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Token representing a cursor into the routines. If not passed, the
        API will return the first page of routines. The token marks the beginning
        of the iterator to be returned and the value of the <code>page_token</code>
        can be accessed at <code>next_page_token</code> of the <code>google.api_core.page_iterator.HTTPIterator</code>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: None
      description: Maximum number of routines to return per page. Defaults to a value
        set by the API.
      id: page_size
      var_type: 'Optional[int] Returns: google.api_core.page_iterator.Iterator: Iterator
        of all <xref uid="google.cloud.bigquery.routine.Routine">Routine</xref>s contained
        within the requested dataset, limited by <code>max_results</code>.'
  type: method
  uid: google.cloud.bigquery.client.Client.list_routines
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_rows
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_rows
  source:
    id: list_rows
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3659
  summary: 'List the rows of the table.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/list


    <aside class="note">

    <b>Note:</b>

    This method assumes that the provided schema is up-to-date with the

    schema as defined on the back-end: if the two schemas are not

    identical, the values returned may be incomplete. To ensure that the

    local copy of the schema is up-to-date, call `client.get_table`.

    </aside>'
  syntax:
    content: 'list_rows(table: typing.Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableListItem,
      google.cloud.bigquery.table.TableReference, str], selected_fields: typing.Optional[typing.Sequence[google.cloud.bigquery.schema.SchemaField]]
      = None, max_results: typing.Optional[int] = None, page_token: typing.Optional[str]
      = None, start_index: typing.Optional[int] = None, page_size: typing.Optional[int]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None)'
    parameters:
    - description: The table to list, or a reference to it. When the table object
        does not contain a schema and <code>selected_fields</code> is not supplied,
        this method calls <code>get_table</code> to fetch the table schema.
      id: table
      var_type: Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        str, ]
    - defaultValue: None
      description: The fields to return. If not supplied, data for all columns are
        downloaded.
      id: selected_fields
      var_type: Sequence[<xref uid="google.cloud.bigquery.schema.SchemaField">google.cloud.bigquery.schema.SchemaField</xref>]
    - defaultValue: None
      description: Maximum number of rows to return.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Token representing a cursor into the table's rows. If not passed,
        the API will return the first page of the rows. The token marks the beginning
        of the iterator to be returned and the value of the <code>page_token</code>
        can be accessed at <code>next_page_token</code> of the <xref uid="google.cloud.bigquery.table.RowIterator">RowIterator</xref>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: None
      description: The zero-based index of the starting row to read.
      id: start_index
      var_type: Optional[int]
    - defaultValue: None
      description: The maximum number of rows in each page of results from this request.
        Non-positive values are ignored. Defaults to a sensible value set by the API.
      id: page_size
      var_type: Optional[int]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>. If multiple requests are made under the hood,
        <code>timeout</code> applies to each individual request.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: 'Iterator of row data <xref uid="google.cloud.bigquery.table.Row">Row</xref>-s.
        During each page, the iterator will have the <code>total_rows</code> attribute
        set, which counts the total number of rows **in the table** (this is distinct
        from the total number of rows in the current page: <code>iterator.page.num_items</code>).'
      var_type: <xref uid="google.cloud.bigquery.table.RowIterator">google.cloud.bigquery.table.RowIterator</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.list_rows
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.list_tables
  langs:
  - python
  module: google.cloud.bigquery.client
  name: list_tables
  source:
    id: list_tables
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1448
  summary: 'List tables in the dataset.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list

    '
  syntax:
    content: 'list_tables(dataset: typing.Union[google.cloud.bigquery.dataset.Dataset,
      google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem,
      str], max_results: typing.Optional[int] = None, page_token: typing.Optional[str]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, page_size: typing.Optional[int] = None)'
    parameters:
    - description: A reference to the dataset whose tables to list from the BigQuery
        API. If a string is passed in, this method attempts to create a dataset reference
        from a string using <xref uid="google.cloud.bigquery.dataset.DatasetReference.from_string">from_string</xref>.
      id: dataset
      var_type: Union[ <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetReference">google.cloud.bigquery.dataset.DatasetReference</xref>,
        <xref uid="google.cloud.bigquery.dataset.DatasetListItem">google.cloud.bigquery.dataset.DatasetListItem</xref>,
        str, ]
    - defaultValue: None
      description: Maximum number of tables to return. Defaults to a value set by
        the API.
      id: max_results
      var_type: Optional[int]
    - defaultValue: None
      description: Token representing a cursor into the tables. If not passed, the
        API will return the first page of tables. The token marks the beginning of
        the iterator to be returned and the value of the <code>page_token</code> can
        be accessed at <code>next_page_token</code> of the <code>google.api_core.page_iterator.HTTPIterator</code>.
      id: page_token
      var_type: Optional[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    - defaultValue: None
      description: Maximum number of tables to return per page. Defaults to a value
        set by the API.
      id: page_size
      var_type: Optional[int]
    returns:
    - description: Iterator of <xref uid="google.cloud.bigquery.table.TableListItem">TableListItem</xref>
        contained within the requested dataset.
      var_type: google.api_core.page_iterator.Iterator
  type: method
  uid: google.cloud.bigquery.client.Client.list_tables
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.load_table_from_dataframe
  langs:
  - python
  module: google.cloud.bigquery.client
  name: load_table_from_dataframe
  source:
    id: load_table_from_dataframe
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2434
  summary: 'Upload the contents of a table from a pandas DataFrame.


    Similar to `load_table_from_uri`, this method creates, starts and

    returns a xref_LoadJob.


    <aside class="note">

    <b>Note:</b>

    REPEATED fields are NOT supported when using the CSV source format.

    They are supported when using the PARQUET source format, but

    due to the way they are encoded in the `parquet` file,

    a mismatch with the existing table schema can occur, so

    REPEATED fields are not properly supported when using `pyarrow<4.0.0`

    using the parquet format.


    https://github.com/googleapis/python-bigquery/issues/19

    </aside>'
  syntax:
    content: "load_table_from_dataframe(\n    dataframe: pandas.DataFrame,\n    destination:\
      \ typing.Union[\n        google.cloud.bigquery.table.Table,\n        google.cloud.bigquery.table.TableReference,\n\
      \        str,\n    ],\n    num_retries: int = 6,\n    job_id: str = None,\n\
      \    job_id_prefix: str = None,\n    location: str = None,\n    project: str\
      \ = None,\n    job_config: google.cloud.bigquery.job.load.LoadJobConfig = None,\n\
      \    parquet_compression: str = \"snappy\",\n    timeout: typing.Union[None,\
      \ float, typing.Tuple[float, float]] = None,\n)"
    exceptions:
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.LoadJob">LoadJob</xref>Config
        class.
      var_type: TypeError
    parameters: []
    returns:
    - description: A new load job.
      var_type: <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.load_table_from_dataframe
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.load_table_from_file
  langs:
  - python
  module: google.cloud.bigquery.client
  name: load_table_from_file
  source:
    id: load_table_from_file
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2329
  summary: 'Upload the contents of this table from a file-like object.


    Similar to `load_table_from_uri`, this method creates, starts and

    returns a xref_LoadJob.

    '
  syntax:
    content: "load_table_from_file(\n    file_obj: typing.IO[bytes],\n    destination:\
      \ typing.Union[\n        google.cloud.bigquery.table.Table,\n        google.cloud.bigquery.table.TableReference,\n\
      \        google.cloud.bigquery.table.TableListItem,\n        str,\n    ],\n\
      \    rewind: bool = False,\n    size: typing.Optional[int] = None,\n    num_retries:\
      \ int = 6,\n    job_id: typing.Optional[str] = None,\n    job_id_prefix: typing.Optional[str]\
      \ = None,\n    location: typing.Optional[str] = None,\n    project: typing.Optional[str]\
      \ = None,\n    job_config: typing.Optional[google.cloud.bigquery.job.load.LoadJobConfig]\
      \ = None,\n    timeout: typing.Union[None, float, typing.Tuple[float, float]]\
      \ = None,\n)"
    exceptions:
    - description: If <code>size</code> is not passed in and can not be determined,
        or if the <code>file_obj</code> can be detected to be a file opened in text
        mode.
      var_type: ValueError
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.LoadJob">LoadJob</xref>Config
        class.
      var_type: TypeError
    parameters: []
    returns:
    - description: A new load job.
      var_type: <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.load_table_from_file
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.load_table_from_json
  langs:
  - python
  module: google.cloud.bigquery.client
  name: load_table_from_json
  source:
    id: load_table_from_json
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2677
  summary: 'Upload the contents of a table from a JSON string or dict.

    '
  syntax:
    content: "load_table_from_json(\n    json_rows: typing.Iterable[typing.Dict[str,\
      \ typing.Any]],\n    destination: typing.Union[\n        google.cloud.bigquery.table.Table,\n\
      \        google.cloud.bigquery.table.TableReference,\n        google.cloud.bigquery.table.TableListItem,\n\
      \        str,\n    ],\n    num_retries: int = 6,\n    job_id: typing.Optional[str]\
      \ = None,\n    job_id_prefix: typing.Optional[str] = None,\n    location: typing.Optional[str]\
      \ = None,\n    project: typing.Optional[str] = None,\n    job_config: typing.Optional[google.cloud.bigquery.job.load.LoadJobConfig]\
      \ = None,\n    timeout: typing.Union[None, float, typing.Tuple[float, float]]\
      \ = None,\n)"
    exceptions:
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.LoadJobConfig">LoadJobConfig</xref>
        class.
      var_type: TypeError
    parameters:
    - description: 'Row data to be inserted. Keys must match the table schema fields
        and values must be JSON-compatible representations. .. note:: If your data
        is already a newline-delimited JSON string, it is best to wrap it into a file-like
        object and pass it to <xref uid="google.cloud.bigquery.client.Client.load_table_from_file">load_table_from_file</xref>::
        import io from google.cloud import bigquery data = u''{"foo": "bar"}'' data_as_file
        = io.StringIO(data) client = bigquery.Client() client.load_table_from_file(data_as_file,
        ...)'
      id: json_rows
      var_type: Iterable[Dict[str, Any]]
    returns:
    - description: A new load job.
      var_type: <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.load_table_from_json
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.load_table_from_uri
  langs:
  - python
  module: google.cloud.bigquery.client
  name: load_table_from_uri
  source:
    id: load_table_from_uri
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 2246
  summary: 'Starts a job for loading data into a table from Cloud Storage.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationload

    '
  syntax:
    content: 'load_table_from_uri(source_uris: typing.Union[str, typing.Sequence[str]],
      destination: typing.Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference,
      google.cloud.bigquery.table.TableListItem, str], job_id: typing.Optional[str]
      = None, job_id_prefix: typing.Optional[str] = None, location: typing.Optional[str]
      = None, project: typing.Optional[str] = None, job_config: typing.Optional[google.cloud.bigquery.job.load.LoadJobConfig]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None)'
    exceptions:
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.LoadJobConfig">LoadJobConfig</xref>
        class.
      var_type: TypeError
    parameters:
    - description: URIs of data files to be loaded; in format <code>gs://<bucket_name>/<object_name_or_glob></code>.
      id: source_uris
      var_type: Union[str, Sequence[str]]
    - description: Table into which data is to be loaded. If a string is passed in,
        this method attempts to create a table reference from a string using <xref
        uid="google.cloud.bigquery.table.TableReference.from_string">from_string</xref>.
      id: destination
      var_type: 'Union[ <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>,
        <xref uid="google.cloud.bigquery.table.TableReference">google.cloud.bigquery.table.TableReference</xref>,
        <xref uid="google.cloud.bigquery.table.TableListItem">google.cloud.bigquery.table.TableListItem</xref>,
        str, ] :keyword job_id: Name of the job. :kwtype job_id: Optional[str] :keyword
        job_id_prefix: The user-provided prefix for a randomly generated job ID. This
        parameter will be ignored if a <code>job_id</code> is also given. :kwtype
        job_id_prefix: Optional[str] :keyword location: Location where to run the
        job. Must match the location of the destination table. :kwtype location: Optional[str]
        :keyword project: Project ID of the project of where to run the job. Defaults
        to the client''s project. :kwtype project: Optional[str] :keyword job_config:
        Extra configuration options for the job. :kwtype job_config: Optional[<xref
        uid="google.cloud.bigquery.job.LoadJobConfig">google.cloud.bigquery.job.LoadJobConfig</xref>]
        :keyword retry: How to retry the RPC. :kwtype retry: Optional[google.api_core.retry.Retry]
        :keyword timeout: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>. :kwtype timeout: Optional[float]'
    returns:
    - description: A new load job.
      var_type: <xref uid="google.cloud.bigquery.job.LoadJob">google.cloud.bigquery.job.LoadJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.load_table_from_uri
- &id001
  attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.location
  langs:
  - python
  module: google.cloud.bigquery.client
  name: location
  source:
    id: location
    path: null
    remote:
      branch: add_goldens
      path: null
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: null
  summary: 'Default location for jobs / datasets / tables.


    '
  syntax: {}
  type: property
  uid: google.cloud.bigquery.client.Client.location
- *id001
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.query
  langs:
  - python
  module: google.cloud.bigquery.client
  name: query
  source:
    id: query
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3197
  summary: 'Run a SQL query.


    See

    https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationquery

    '
  syntax:
    content: 'query(query: str, job_config: typing.Optional[google.cloud.bigquery.job.query.QueryJobConfig]
      = None, job_id: typing.Optional[str] = None, job_id_prefix: typing.Optional[str]
      = None, location: typing.Optional[str] = None, project: typing.Optional[str]
      = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>,
      timeout: typing.Optional[float] = None, job_retry: google.api_core.retry.Retry
      = <google.api_core.retry.Retry object>, api_method: typing.Union[str, google.cloud.bigquery.enums.QueryApiMethod]
      = QueryApiMethod.INSERT)'
    exceptions:
    - description: If <code>job_config</code> is not an instance of <xref uid="google.cloud.bigquery.job.QueryJobConfig">QueryJobConfig</xref>
        class, or if both <code>job_id</code> and non-<code>None</code> non-default
        <code>job_retry</code> are provided.
      var_type: TypeError
    parameters:
    - description: SQL query to be executed. Defaults to the standard SQL dialect.
        Use the <code>job_config</code> parameter to change dialects.
      id: query
      var_type: 'str :keyword job_config: Extra configuration options for the job.
        To override any options that were previously set in the <code>default_query_job_config</code>
        given to the <code>Client</code> constructor, manually set those options to
        <code>None</code>, or whatever value is preferred. :kwtype job_config: Optional[<xref
        uid="google.cloud.bigquery.job.QueryJobConfig">google.cloud.bigquery.job.QueryJobConfig</xref>]
        :keyword job_id: ID to use for the query job. :kwtype job_id: Optional[str]
        :keyword job_id_prefix: The prefix to use for a randomly generated job ID.
        This parameter will be ignored if a <code>job_id</code> is also given. :kwtype
        job_id_prefix: Optional[str] :keyword location: Location where to run the
        job. Must match the location of the table used in the query as well as the
        destination table. :kwtype location: Optional[str] :keyword project: Project
        ID of the project of where to run the job. Defaults to the client''s project.
        :kwtype project: Optional[str] :keyword retry: How to retry the RPC. This
        only applies to making RPC calls. It isn''t used to retry failed jobs. This
        has a reasonable default that should only be overridden with care. :kwtype
        retry: Optional[google.api_core.retry.Retry] :keyword timeout: The number
        of seconds to wait for the underlying HTTP transport before using <code>retry</code>.
        :kwtype timeout: Optional[float] :keyword job_retry: How to retry failed jobs.
        The default retries rate-limit-exceeded errors. Passing <code>None</code>
        disables job retry. Not all jobs can be retried. If <code>job_id</code> is
        provided, then the job returned by the query will not be retryable, and an
        exception will be raised if a non-<code>None</code> (and non-default) value
        for <code>job_retry</code> is also provided. Note that errors aren''t detected
        until <code>result()</code> is called on the job returned. The <code>job_retry</code>
        specified here becomes the default <code>job_retry</code> for <code>result()</code>,
        where it can also be specified. :kwtype job_retry: Optional[google.api_core.retry.Retry]
        :keyword api_method: Method with which to start the query job. See <xref uid="google.cloud.bigquery.enums.QueryApiMethod">QueryApiMethod</xref>
        for details on the difference between the query start methods. :kwtype api_method:
        Union[str, enums.QueryApiMethod]'
    returns:
    - description: A new query job instance.
      var_type: <xref uid="google.cloud.bigquery.job.QueryJob">google.cloud.bigquery.job.QueryJob</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.query
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.schema_from_json
  langs:
  - python
  module: google.cloud.bigquery.client
  name: schema_from_json
  source:
    id: schema_from_json
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3866
  summary: 'Takes a file object or file path that contains json that describes

    a table schema.

    '
  syntax:
    content: 'schema_from_json(file_or_path: PathType)'
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.schema_from_json
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.schema_to_json
  langs:
  - python
  module: google.cloud.bigquery.client
  name: schema_to_json
  source:
    id: schema_to_json
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 3879
  summary: 'Takes a list of schema field objects.


    Serializes the list of schema field objects as json to a file.


    Destination is a file path or a file object.


    '
  syntax:
    content: "schema_to_json(\n    schema_list: typing.Sequence[google.cloud.bigquery.schema.SchemaField],\n\
      \    destination: PathType,\n)"
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.schema_to_json
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.update_dataset
  langs:
  - python
  module: google.cloud.bigquery.client
  name: update_dataset
  source:
    id: update_dataset
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1021
  summary: 'Change some fields of a dataset.


    Use `fields` to specify which fields to update. At least one field

    must be provided. If a field is listed in `fields` and is `None` in

    `dataset`, it will be deleted.


    If `dataset.etag` is not `None`, the update will only

    succeed if the dataset on the server has the same ETag. Thus

    reading a dataset with `get_dataset`, changing its fields,

    and then passing it to `update_dataset` will ensure that the changes

    will only be saved if no modifications to the dataset occurred

    since the read.

    '
  syntax:
    content: 'update_dataset(dataset: google.cloud.bigquery.dataset.Dataset, fields:
      typing.Sequence[str], retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    parameters:
    - description: The dataset to update.
      id: dataset
      var_type: <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>
    - description: 'The properties of <code>dataset</code> to change. These are strings
        corresponding to the properties of <xref uid="google.cloud.bigquery.dataset.Dataset">Dataset</xref>.
        For example, to update the default expiration times, specify both properties
        in the <code>fields</code> argument: .. code-block:: python bigquery_client.update_dataset(
        dataset, [ "default_partition_expiration_ms", "default_table_expiration_ms",
        ] )'
      id: fields
      var_type: Sequence[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: How to retry the RPC.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: The modified <code>Dataset</code> instance.
      var_type: <xref uid="google.cloud.bigquery.dataset.Dataset">google.cloud.bigquery.dataset.Dataset</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.update_dataset
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.update_model
  langs:
  - python
  module: google.cloud.bigquery.client
  name: update_model
  source:
    id: update_model
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1091
  summary: '[Beta] Change some fields of a model.


    Use `fields` to specify which fields to update. At least one field

    must be provided. If a field is listed in `fields` and is `None`

    in `model`, the field value will be deleted.


    If `model.etag` is not `None`, the update will only succeed if

    the model on the server has the same ETag. Thus reading a model with

    `get_model`, changing its fields, and then passing it to

    `update_model` will ensure that the changes will only be saved if

    no modifications to the model occurred since the read.

    '
  syntax:
    content: 'update_model(model: google.cloud.bigquery.model.Model, fields: typing.Sequence[str],
      retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>, timeout:
      typing.Optional[float] = None)'
    parameters:
    - description: The model to update.
      id: model
      var_type: <xref uid="google.cloud.bigquery.model.Model">google.cloud.bigquery.model.Model</xref>
    - description: 'The properties of <code>model</code> to change. These are strings
        corresponding to the properties of <xref uid="google.cloud.bigquery.model.Model">Model</xref>.
        For example, to update the descriptive properties of the model, specify them
        in the <code>fields</code> argument: .. code-block:: python bigquery_client.update_model(
        model, ["description", "friendly_name"] )'
      id: fields
      var_type: Sequence[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: A description of how to retry the API call.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: The model resource returned from the API call.
      var_type: <xref uid="google.cloud.bigquery.model.Model">google.cloud.bigquery.model.Model</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.update_model
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.update_routine
  langs:
  - python
  module: google.cloud.bigquery.client
  name: update_routine
  source:
    id: update_routine
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1155
  summary: '[Beta] Change some fields of a routine.


    Use `fields` to specify which fields to update. At least one field

    must be provided. If a field is listed in `fields` and is `None`

    in `routine`, the field value will be deleted.


    <aside class="warning">

    <b>Warning:</b>

    During beta, partial updates are not supported. You must provide

    all fields in the resource.

    </aside>

    If xref_etag is not

    `None`, the update will only succeed if the resource on the server

    has the same ETag. Thus reading a routine with

    xref_get_routine, changing

    its fields, and then passing it to this method will ensure that the

    changes will only be saved if no modifications to the resource

    occurred since the read.

    '
  syntax:
    content: 'update_routine(routine: google.cloud.bigquery.routine.routine.Routine,
      fields: typing.Sequence[str], retry: google.api_core.retry.Retry = <google.api_core.retry.Retry
      object>, timeout: typing.Optional[float] = None)'
    parameters:
    - description: The routine to update.
      id: routine
      var_type: <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>
    - description: 'The fields of <code>routine</code> to change, spelled as the <xref
        uid="google.cloud.bigquery.routine.Routine">Routine</xref> properties. For
        example, to update the description property of the routine, specify it in
        the <code>fields</code> argument: .. code-block:: python bigquery_client.update_routine(
        routine, ["description"] )'
      id: fields
      var_type: Sequence[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: A description of how to retry the API call.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: The routine resource returned from the API call.
      var_type: <xref uid="google.cloud.bigquery.routine.Routine">google.cloud.bigquery.routine.Routine</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.update_routine
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.update_table
  langs:
  - python
  module: google.cloud.bigquery.client
  name: update_table
  source:
    id: update_table
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 1229
  summary: 'Change some fields of a table.


    Use `fields` to specify which fields to update. At least one field

    must be provided. If a field is listed in `fields` and is `None`

    in `table`, the field value will be deleted.


    If `table.etag` is not `None`, the update will only succeed if

    the table on the server has the same ETag. Thus reading a table with

    `get_table`, changing its fields, and then passing it to

    `update_table` will ensure that the changes will only be saved if

    no modifications to the table occurred since the read.

    '
  syntax:
    content: 'update_table(table: google.cloud.bigquery.table.Table, fields: typing.Sequence[str],
      retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object>, timeout:
      typing.Optional[float] = None)'
    parameters:
    - description: The table to update.
      id: table
      var_type: <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>
    - description: 'The fields of <code>table</code> to change, spelled as the <xref
        uid="google.cloud.bigquery.table.Table">Table</xref> properties. For example,
        to update the descriptive properties of the table, specify them in the <code>fields</code>
        argument: .. code-block:: python bigquery_client.update_table( table, ["description",
        "friendly_name"] )'
      id: fields
      var_type: Sequence[str]
    - defaultValue: <Retry predicate=<function _should_retry at 0x7fd9179a2a60>, initial=1.0,
        maximum=60.0, multiplier=2.0, deadline=600.0, on_error=None>
      description: A description of how to retry the API call.
      id: retry
      var_type: Optional[google.api_core.retry.Retry]
    - defaultValue: None
      description: The number of seconds to wait for the underlying HTTP transport
        before using <code>retry</code>.
      id: timeout
      var_type: Optional[float]
    returns:
    - description: The table resource returned from the API call.
      var_type: <xref uid="google.cloud.bigquery.table.Table">google.cloud.bigquery.table.Table</xref>
  type: method
  uid: google.cloud.bigquery.client.Client.update_table
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.__init__
  langs:
  - python
  module: google.cloud.bigquery.client
  name: __init__
  source:
    id: __init__
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 216
  summary: 'Initialize self.  See help(type(self)) for accurate signature.


    '
  syntax:
    content: "__init__(\n    project=None,\n    credentials=None,\n    _http=None,\n\
      \    location=None,\n    default_query_job_config=None,\n    client_info=None,\n\
      \    client_options=None,\n)"
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.__init__
- class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.get_iam_policy
  langs:
  - python
  module: google.cloud.bigquery.client
  name: get_iam_policy
  source:
    id: get_iam_policy
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 806
  summary: API documentation for `bigquery.client.Client.get_iam_policy` method.
  syntax:
    content: get_iam_policy(table, requested_policy_version=1, retry=<google.api_core.retry.Retry
      object>, timeout=None)
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.get_iam_policy
- class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.set_iam_policy
  langs:
  - python
  module: google.cloud.bigquery.client
  name: set_iam_policy
  source:
    id: set_iam_policy
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 834
  summary: API documentation for `bigquery.client.Client.set_iam_policy` method.
  syntax:
    content: set_iam_policy(table, policy, updateMask=None, retry=<google.api_core.retry.Retry
      object>, timeout=None)
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.set_iam_policy
- class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client.test_iam_permissions
  langs:
  - python
  module: google.cloud.bigquery.client
  name: test_iam_permissions
  source:
    id: test_iam_permissions
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 867
  summary: API documentation for `bigquery.client.Client.test_iam_permissions` method.
  syntax:
    content: test_iam_permissions(table, permissions, retry=<google.api_core.retry.Retry
      object>, timeout=None)
    parameters: []
  type: method
  uid: google.cloud.bigquery.client.Client.test_iam_permissions
- attributes: []
  class: google.cloud.bigquery.client.Client
  fullName: google.cloud.bigquery.client.Client
  inheritance:
  - inheritance:
    - inheritance:
      - inheritance:
        - type: builtins.object
        type: google.cloud.client._ClientFactoryMixin
      type: google.cloud.client.Client
    - inheritance:
      - type: builtins.object
      type: google.cloud.client._ClientProjectMixin
    type: google.cloud.client.ClientWithProject
  langs:
  - python
  module: google.cloud.bigquery.client
  name: Client
  source:
    id: Client
    path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
    remote:
      branch: add_goldens
      path: tests/testdata/gapic-combo/google/cloud/bigquery/client.py
      repo: git@github.com:googleapis/sphinx-docfx-yaml.git
    startLine: 170
  summary: 'Client to bundle configuration needed for API requests.

    '
  syntax:
    content: "Client(\n    project=None,\n    credentials=None,\n    _http=None,\n\
      \    location=None,\n    default_query_job_config=None,\n    client_info=None,\n\
      \    client_options=None,\n)"
    exceptions:
    - description: Raised if <code>credentials</code> is not specified and the library
        fails to acquire default credentials.
      var_type: google.auth.exceptions.DefaultCredentialsError
    parameters:
    - description: Project ID for the project which the client acts on behalf of.
        Will be passed when creating a dataset / job. If not passed, falls back to
        the default inferred from the environment.
      id: project
      var_type: Optional[str]
    - description: The OAuth2 Credentials to use for this client. If not passed (and
        if no <code>_http</code> object is passed), falls back to the default inferred
        from the environment.
      id: credentials
      var_type: Optional[google.auth.credentials.Credentials]
    - description: HTTP object to make requests. Can be any object that defines <code>request()</code>
        with the same interface as <code>requests.Session.request</code>. If not passed,
        an <code>_http</code> object is created that is bound to the <code>credentials</code>
        for the current object. This parameter should be considered private, and could
        change in the future.
      id: _http
      var_type: Optional[requests.Session]
    - description: Default location for jobs / datasets / tables.
      id: location
      var_type: Optional[str]
    - description: Default <code>QueryJobConfig</code>. Will be merged into job configs
        passed into the <code>query</code> method.
      id: default_query_job_config
      var_type: Optional[<xref uid="google.cloud.bigquery.job.QueryJobConfig">google.cloud.bigquery.job.QueryJobConfig</xref>]
    - description: The client info used to send a user-agent string along with API
        requests. If <code>None</code>, then default info will be used. Generally,
        you only need to set this if you're developing your own library or partner
        tool.
      id: client_info
      var_type: Optional[google.api_core.client_info.ClientInfo]
    - description: Client options used to set user options on the client. API Endpoint
        should be set through client_options.
      id: client_options
      var_type: Optional[Union[google.api_core.client_options.ClientOptions, Dict]]
  type: method
  uid: google.cloud.bigquery.client.Client
references:
- fullName: google.cloud.bigquery.client.Client.SCOPE
  isExternal: false
  name: SCOPE
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.SCOPE
- fullName: google.cloud.bigquery.client.Client.__getstate__
  isExternal: false
  name: __getstate__
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.__getstate__
- fullName: google.cloud.bigquery.client.Client.cancel_job
  isExternal: false
  name: cancel_job
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.cancel_job
- fullName: google.cloud.bigquery.client.Client.close
  isExternal: false
  name: close
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.close
- fullName: google.cloud.bigquery.client.Client.copy_table
  isExternal: false
  name: copy_table
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.copy_table
- fullName: google.cloud.bigquery.client.Client.create_dataset
  isExternal: false
  name: create_dataset
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.create_dataset
- fullName: google.cloud.bigquery.client.Client.create_job
  isExternal: false
  name: create_job
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.create_job
- fullName: google.cloud.bigquery.client.Client.create_routine
  isExternal: false
  name: create_routine
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.create_routine
- fullName: google.cloud.bigquery.client.Client.create_table
  isExternal: false
  name: create_table
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.create_table
- fullName: google.cloud.bigquery.client.Client.dataset
  isExternal: false
  name: dataset
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.dataset
- fullName: google.cloud.bigquery.client.Client.delete_dataset
  isExternal: false
  name: delete_dataset
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.delete_dataset
- fullName: google.cloud.bigquery.client.Client.delete_job_metadata
  isExternal: false
  name: delete_job_metadata
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.delete_job_metadata
- fullName: google.cloud.bigquery.client.Client.delete_model
  isExternal: false
  name: delete_model
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.delete_model
- fullName: google.cloud.bigquery.client.Client.delete_routine
  isExternal: false
  name: delete_routine
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.delete_routine
- fullName: google.cloud.bigquery.client.Client.delete_table
  isExternal: false
  name: delete_table
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.delete_table
- fullName: google.cloud.bigquery.client.Client.extract_table
  isExternal: false
  name: extract_table
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.extract_table
- fullName: google.cloud.bigquery.client.Client.from_service_account_info
  isExternal: false
  name: from_service_account_info
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.from_service_account_info
- fullName: google.cloud.bigquery.client.Client.from_service_account_json
  isExternal: false
  name: from_service_account_json
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.from_service_account_json
- fullName: google.cloud.bigquery.client.Client.get_dataset
  isExternal: false
  name: get_dataset
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_dataset
- fullName: google.cloud.bigquery.client.Client.get_job
  isExternal: false
  name: get_job
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_job
- fullName: google.cloud.bigquery.client.Client.get_model
  isExternal: false
  name: get_model
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_model
- fullName: google.cloud.bigquery.client.Client.get_routine
  isExternal: false
  name: get_routine
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_routine
- fullName: google.cloud.bigquery.client.Client.get_service_account_email
  isExternal: false
  name: get_service_account_email
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_service_account_email
- fullName: google.cloud.bigquery.client.Client.get_table
  isExternal: false
  name: get_table
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_table
- fullName: google.cloud.bigquery.client.Client.insert_rows
  isExternal: false
  name: insert_rows
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.insert_rows
- fullName: google.cloud.bigquery.client.Client.insert_rows_from_dataframe
  isExternal: false
  name: insert_rows_from_dataframe
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.insert_rows_from_dataframe
- fullName: google.cloud.bigquery.client.Client.insert_rows_json
  isExternal: false
  name: insert_rows_json
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.insert_rows_json
- fullName: google.cloud.bigquery.client.Client.job_from_resource
  isExternal: false
  name: job_from_resource
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.job_from_resource
- fullName: google.cloud.bigquery.client.Client.list_datasets
  isExternal: false
  name: list_datasets
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_datasets
- fullName: google.cloud.bigquery.client.Client.list_jobs
  isExternal: false
  name: list_jobs
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_jobs
- fullName: google.cloud.bigquery.client.Client.list_models
  isExternal: false
  name: list_models
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_models
- fullName: google.cloud.bigquery.client.Client.list_partitions
  isExternal: false
  name: list_partitions
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_partitions
- fullName: google.cloud.bigquery.client.Client.list_projects
  isExternal: false
  name: list_projects
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_projects
- fullName: google.cloud.bigquery.client.Client.list_routines
  isExternal: false
  name: list_routines
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_routines
- fullName: google.cloud.bigquery.client.Client.list_rows
  isExternal: false
  name: list_rows
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_rows
- fullName: google.cloud.bigquery.client.Client.list_tables
  isExternal: false
  name: list_tables
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.list_tables
- fullName: google.cloud.bigquery.client.Client.load_table_from_dataframe
  isExternal: false
  name: load_table_from_dataframe
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.load_table_from_dataframe
- fullName: google.cloud.bigquery.client.Client.load_table_from_file
  isExternal: false
  name: load_table_from_file
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.load_table_from_file
- fullName: google.cloud.bigquery.client.Client.load_table_from_json
  isExternal: false
  name: load_table_from_json
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.load_table_from_json
- fullName: google.cloud.bigquery.client.Client.load_table_from_uri
  isExternal: false
  name: load_table_from_uri
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.load_table_from_uri
- fullName: google.cloud.bigquery.client.Client.location
  isExternal: false
  name: location
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.location
- fullName: google.cloud.bigquery.client.Client.query
  isExternal: false
  name: query
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.query
- fullName: google.cloud.bigquery.client.Client.schema_from_json
  isExternal: false
  name: schema_from_json
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.schema_from_json
- fullName: google.cloud.bigquery.client.Client.schema_to_json
  isExternal: false
  name: schema_to_json
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.schema_to_json
- fullName: google.cloud.bigquery.client.Client.update_dataset
  isExternal: false
  name: update_dataset
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.update_dataset
- fullName: google.cloud.bigquery.client.Client.update_model
  isExternal: false
  name: update_model
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.update_model
- fullName: google.cloud.bigquery.client.Client.update_routine
  isExternal: false
  name: update_routine
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.update_routine
- fullName: google.cloud.bigquery.client.Client.update_table
  isExternal: false
  name: update_table
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.update_table
- fullName: google.cloud.bigquery.client.Client.__init__
  isExternal: false
  name: __init__
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.__init__
- fullName: google.cloud.bigquery.client.Client.get_iam_policy
  isExternal: false
  name: get_iam_policy
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.get_iam_policy
- fullName: google.cloud.bigquery.client.Client.set_iam_policy
  isExternal: false
  name: set_iam_policy
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.set_iam_policy
- fullName: google.cloud.bigquery.client.Client.test_iam_permissions
  isExternal: false
  name: test_iam_permissions
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client.test_iam_permissions
- fullName: google.cloud.bigquery.client.Client
  isExternal: false
  name: Client
  parent: google.cloud.bigquery.client.Client
  uid: google.cloud.bigquery.client.Client
